{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import numpy as np  \n",
    "import torch.nn.functional as F  \n",
    "from pytorch_grad_cam import GradCAM  \n",
    "import shap  \n",
    "import matplotlib.pyplot as plt  \n",
    "from model import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[-0.5889067 ]\n",
      "    [-0.56582094]\n",
      "    [-0.54403773]\n",
      "    ...\n",
      "    [-0.56337926]\n",
      "    [-0.57372922]\n",
      "    [-0.58394798]]\n",
      "\n",
      "   [[-0.56309448]\n",
      "    [-0.5628318 ]\n",
      "    [-0.56300838]\n",
      "    ...\n",
      "    [-0.56690623]\n",
      "    [-0.56694305]\n",
      "    [-0.567388  ]]\n",
      "\n",
      "   [[-0.56105258]\n",
      "    [-0.5672651 ]\n",
      "    [-0.57330655]\n",
      "    ...\n",
      "    [-0.56665985]\n",
      "    [-0.56003193]\n",
      "    [-0.55331294]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57589625]\n",
      "    [-0.56192421]\n",
      "    [-0.54889107]\n",
      "    ...\n",
      "    [-0.570832  ]\n",
      "    [-0.55805765]\n",
      "    [-0.54490154]]\n",
      "\n",
      "   [[-0.58557934]\n",
      "    [-0.56732206]\n",
      "    [-0.54964627]\n",
      "    ...\n",
      "    [-0.56640397]\n",
      "    [-0.54791863]\n",
      "    [-0.52939389]]\n",
      "\n",
      "   [[-0.5413599 ]\n",
      "    [-0.56063661]\n",
      "    [-0.57900903]\n",
      "    ...\n",
      "    [-0.5678143 ]\n",
      "    [-0.54962892]\n",
      "    [-0.53127093]]]\n",
      "\n",
      "\n",
      "  [[[-0.57691006]\n",
      "    [-0.5761982 ]\n",
      "    [-0.57515739]\n",
      "    ...\n",
      "    [-0.56555154]\n",
      "    [-0.56277122]\n",
      "    [-0.56022342]]\n",
      "\n",
      "   [[-0.56468569]\n",
      "    [-0.57447308]\n",
      "    [-0.58390754]\n",
      "    ...\n",
      "    [-0.56236351]\n",
      "    [-0.55721601]\n",
      "    [-0.55196328]]\n",
      "\n",
      "   [[-0.54658876]\n",
      "    [-0.55844092]\n",
      "    [-0.57009112]\n",
      "    ...\n",
      "    [-0.56407535]\n",
      "    [-0.5718381 ]\n",
      "    [-0.57965592]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57330264]\n",
      "    [-0.57285341]\n",
      "    [-0.57265814]\n",
      "    ...\n",
      "    [-0.56518439]\n",
      "    [-0.56569836]\n",
      "    [-0.56651245]]\n",
      "\n",
      "   [[-0.57969417]\n",
      "    [-0.57315726]\n",
      "    [-0.56688039]\n",
      "    ...\n",
      "    [-0.5651599 ]\n",
      "    [-0.56903135]\n",
      "    [-0.57285474]]\n",
      "\n",
      "   [[-0.563354  ]\n",
      "    [-0.56133177]\n",
      "    [-0.55948533]\n",
      "    ...\n",
      "    [-0.56862434]\n",
      "    [-0.5609267 ]\n",
      "    [-0.55252283]]]\n",
      "\n",
      "\n",
      "  [[[-0.59551462]\n",
      "    [-0.59164595]\n",
      "    [-0.58754363]\n",
      "    ...\n",
      "    [-0.56454786]\n",
      "    [-0.56935978]\n",
      "    [-0.57477552]]\n",
      "\n",
      "   [[-0.57364808]\n",
      "    [-0.56828329]\n",
      "    [-0.56335683]\n",
      "    ...\n",
      "    [-0.56419963]\n",
      "    [-0.56701829]\n",
      "    [-0.56994836]]\n",
      "\n",
      "   [[-0.58410963]\n",
      "    [-0.56624273]\n",
      "    [-0.54944162]\n",
      "    ...\n",
      "    [-0.55922804]\n",
      "    [-0.56771413]\n",
      "    [-0.57629986]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56397294]\n",
      "    [-0.57102097]\n",
      "    [-0.57783339]\n",
      "    ...\n",
      "    [-0.56208508]\n",
      "    [-0.57393367]\n",
      "    [-0.5860908 ]]\n",
      "\n",
      "   [[-0.52270711]\n",
      "    [-0.53926657]\n",
      "    [-0.55541382]\n",
      "    ...\n",
      "    [-0.56741946]\n",
      "    [-0.56531649]\n",
      "    [-0.56335648]]\n",
      "\n",
      "   [[-0.57029731]\n",
      "    [-0.57564779]\n",
      "    [-0.58073325]\n",
      "    ...\n",
      "    [-0.55968523]\n",
      "    [-0.56510609]\n",
      "    [-0.5702774 ]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[-0.50747687]\n",
      "    [-0.52637436]\n",
      "    [-0.54517895]\n",
      "    ...\n",
      "    [-0.56225847]\n",
      "    [-0.54913095]\n",
      "    [-0.5355984 ]]\n",
      "\n",
      "   [[-0.58857892]\n",
      "    [-0.57103159]\n",
      "    [-0.55401049]\n",
      "    ...\n",
      "    [-0.56556384]\n",
      "    [-0.56727132]\n",
      "    [-0.56906677]]\n",
      "\n",
      "   [[-0.54529622]\n",
      "    [-0.55494664]\n",
      "    [-0.56437939]\n",
      "    ...\n",
      "    [-0.56474534]\n",
      "    [-0.55284994]\n",
      "    [-0.54023054]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57797348]\n",
      "    [-0.57587254]\n",
      "    [-0.57392887]\n",
      "    ...\n",
      "    [-0.56568919]\n",
      "    [-0.56727642]\n",
      "    [-0.56921731]]\n",
      "\n",
      "   [[-0.55707219]\n",
      "    [-0.56443949]\n",
      "    [-0.57158225]\n",
      "    ...\n",
      "    [-0.56409058]\n",
      "    [-0.57195568]\n",
      "    [-0.58033477]]\n",
      "\n",
      "   [[-0.5830099 ]\n",
      "    [-0.58200535]\n",
      "    [-0.58060227]\n",
      "    ...\n",
      "    [-0.56361232]\n",
      "    [-0.55473398]\n",
      "    [-0.54576149]]]\n",
      "\n",
      "\n",
      "  [[[-0.56858239]\n",
      "    [-0.56728651]\n",
      "    [-0.56569616]\n",
      "    ...\n",
      "    [-0.56829169]\n",
      "    [-0.56034522]\n",
      "    [-0.55257532]]\n",
      "\n",
      "   [[-0.57340382]\n",
      "    [-0.57023438]\n",
      "    [-0.56738475]\n",
      "    ...\n",
      "    [-0.56191367]\n",
      "    [-0.57049996]\n",
      "    [-0.57957032]]\n",
      "\n",
      "   [[-0.56142885]\n",
      "    [-0.57077827]\n",
      "    [-0.58009033]\n",
      "    ...\n",
      "    [-0.5612224 ]\n",
      "    [-0.55940289]\n",
      "    [-0.55726087]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58305894]\n",
      "    [-0.58003165]\n",
      "    [-0.5765955 ]\n",
      "    ...\n",
      "    [-0.56676842]\n",
      "    [-0.56519386]\n",
      "    [-0.56366069]]\n",
      "\n",
      "   [[-0.55949513]\n",
      "    [-0.56422277]\n",
      "    [-0.56833085]\n",
      "    ...\n",
      "    [-0.56366295]\n",
      "    [-0.56580679]\n",
      "    [-0.56827449]]\n",
      "\n",
      "   [[-0.53397485]\n",
      "    [-0.54780863]\n",
      "    [-0.56143432]\n",
      "    ...\n",
      "    [-0.56294037]\n",
      "    [-0.57885293]\n",
      "    [-0.59470598]]]\n",
      "\n",
      "\n",
      "  [[[-0.58449737]\n",
      "    [-0.57945171]\n",
      "    [-0.57453264]\n",
      "    ...\n",
      "    [-0.55901067]\n",
      "    [-0.54333617]\n",
      "    [-0.52738562]]\n",
      "\n",
      "   [[-0.57200739]\n",
      "    [-0.56418771]\n",
      "    [-0.5570127 ]\n",
      "    ...\n",
      "    [-0.56255446]\n",
      "    [-0.54850838]\n",
      "    [-0.53424683]]\n",
      "\n",
      "   [[-0.57517528]\n",
      "    [-0.5697946 ]\n",
      "    [-0.5646329 ]\n",
      "    ...\n",
      "    [-0.5636788 ]\n",
      "    [-0.55849015]\n",
      "    [-0.55295488]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.59953372]\n",
      "    [-0.58477733]\n",
      "    [-0.57014028]\n",
      "    ...\n",
      "    [-0.56542515]\n",
      "    [-0.56479388]\n",
      "    [-0.56428479]]\n",
      "\n",
      "   [[-0.57828935]\n",
      "    [-0.56179269]\n",
      "    [-0.54574266]\n",
      "    ...\n",
      "    [-0.57058462]\n",
      "    [-0.56188516]\n",
      "    [-0.55281457]]\n",
      "\n",
      "   [[-0.57033667]\n",
      "    [-0.56932693]\n",
      "    [-0.56825841]\n",
      "    ...\n",
      "    [-0.56700114]\n",
      "    [-0.56303748]\n",
      "    [-0.5587242 ]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-0.59012018]\n",
      "    [-0.5830496 ]\n",
      "    [-0.57589328]\n",
      "    ...\n",
      "    [-0.60612138]\n",
      "    [-0.58484517]\n",
      "    [-0.56120165]]\n",
      "\n",
      "   [[-0.58679995]\n",
      "    [-0.57746091]\n",
      "    [-0.56832714]\n",
      "    ...\n",
      "    [-0.56041371]\n",
      "    [-0.56404362]\n",
      "    [-0.56752098]]\n",
      "\n",
      "   [[-0.58105761]\n",
      "    [-0.57451374]\n",
      "    [-0.56766222]\n",
      "    ...\n",
      "    [-0.56653796]\n",
      "    [-0.5660493 ]\n",
      "    [-0.5655858 ]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.55342845]\n",
      "    [-0.55217869]\n",
      "    [-0.55139668]\n",
      "    ...\n",
      "    [-0.56568881]\n",
      "    [-0.57197212]\n",
      "    [-0.57842318]]\n",
      "\n",
      "   [[-0.55485826]\n",
      "    [-0.55865814]\n",
      "    [-0.56273699]\n",
      "    ...\n",
      "    [-0.55867781]\n",
      "    [-0.54856357]\n",
      "    [-0.53794667]]\n",
      "\n",
      "   [[-0.59632333]\n",
      "    [-0.58571004]\n",
      "    [-0.57510987]\n",
      "    ...\n",
      "    [-0.53640619]\n",
      "    [-0.55997758]\n",
      "    [-0.58455598]]]\n",
      "\n",
      "\n",
      "  [[[-0.57360249]\n",
      "    [-0.55144208]\n",
      "    [-0.53016522]\n",
      "    ...\n",
      "    [-0.54409348]\n",
      "    [-0.53057189]\n",
      "    [-0.51705745]]\n",
      "\n",
      "   [[-0.56707246]\n",
      "    [-0.58048874]\n",
      "    [-0.59332444]\n",
      "    ...\n",
      "    [-0.53617978]\n",
      "    [-0.55118743]\n",
      "    [-0.56748787]]\n",
      "\n",
      "   [[-0.58425592]\n",
      "    [-0.56821036]\n",
      "    [-0.55332515]\n",
      "    ...\n",
      "    [-0.55301737]\n",
      "    [-0.54647689]\n",
      "    [-0.54016744]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56283553]\n",
      "    [-0.56195389]\n",
      "    [-0.56161706]\n",
      "    ...\n",
      "    [-0.54676323]\n",
      "    [-0.5589445 ]\n",
      "    [-0.57201261]]\n",
      "\n",
      "   [[-0.57267544]\n",
      "    [-0.56847515]\n",
      "    [-0.5644409 ]\n",
      "    ...\n",
      "    [-0.5844917 ]\n",
      "    [-0.58348756]\n",
      "    [-0.58219041]]\n",
      "\n",
      "   [[-0.52529347]\n",
      "    [-0.54314114]\n",
      "    [-0.55989282]\n",
      "    ...\n",
      "    [-0.5769709 ]\n",
      "    [-0.59200611]\n",
      "    [-0.60669046]]]\n",
      "\n",
      "\n",
      "  [[[-0.58600994]\n",
      "    [-0.57888392]\n",
      "    [-0.57181795]\n",
      "    ...\n",
      "    [-0.5060538 ]\n",
      "    [-0.54737727]\n",
      "    [-0.59088889]]\n",
      "\n",
      "   [[-0.57769752]\n",
      "    [-0.56387307]\n",
      "    [-0.55085383]\n",
      "    ...\n",
      "    [-0.55314904]\n",
      "    [-0.56159625]\n",
      "    [-0.57005903]]\n",
      "\n",
      "   [[-0.55152191]\n",
      "    [-0.54283216]\n",
      "    [-0.53514797]\n",
      "    ...\n",
      "    [-0.53740746]\n",
      "    [-0.52875219]\n",
      "    [-0.5203582 ]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57501056]\n",
      "    [-0.56100777]\n",
      "    [-0.54753545]\n",
      "    ...\n",
      "    [-0.54936348]\n",
      "    [-0.56564294]\n",
      "    [-0.58211996]]\n",
      "\n",
      "   [[-0.56892521]\n",
      "    [-0.55310775]\n",
      "    [-0.53796562]\n",
      "    ...\n",
      "    [-0.57166859]\n",
      "    [-0.57917291]\n",
      "    [-0.58644522]]\n",
      "\n",
      "   [[-0.58608322]\n",
      "    [-0.5714572 ]\n",
      "    [-0.55747146]\n",
      "    ...\n",
      "    [-0.57262368]\n",
      "    [-0.57453999]\n",
      "    [-0.57596377]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[-0.5658623 ]\n",
      "    [-0.57212835]\n",
      "    [-0.57859627]\n",
      "    ...\n",
      "    [-0.60401437]\n",
      "    [-0.59896744]\n",
      "    [-0.5928319 ]]\n",
      "\n",
      "   [[-0.56965088]\n",
      "    [-0.56153193]\n",
      "    [-0.55318732]\n",
      "    ...\n",
      "    [-0.57024763]\n",
      "    [-0.55828006]\n",
      "    [-0.54541255]]\n",
      "\n",
      "   [[-0.54845908]\n",
      "    [-0.55095366]\n",
      "    [-0.55315213]\n",
      "    ...\n",
      "    [-0.57046967]\n",
      "    [-0.57318593]\n",
      "    [-0.5755202 ]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58931495]\n",
      "    [-0.56488313]\n",
      "    [-0.54136596]\n",
      "    ...\n",
      "    [-0.56429797]\n",
      "    [-0.56257565]\n",
      "    [-0.56098901]]\n",
      "\n",
      "   [[-0.57958844]\n",
      "    [-0.55909447]\n",
      "    [-0.53978471]\n",
      "    ...\n",
      "    [-0.55376039]\n",
      "    [-0.5519666 ]\n",
      "    [-0.55043165]]\n",
      "\n",
      "   [[-0.57830309]\n",
      "    [-0.57729852]\n",
      "    [-0.57644299]\n",
      "    ...\n",
      "    [-0.54153826]\n",
      "    [-0.55898217]\n",
      "    [-0.57749909]]]\n",
      "\n",
      "\n",
      "  [[[-0.56202959]\n",
      "    [-0.57454907]\n",
      "    [-0.58687017]\n",
      "    ...\n",
      "    [-0.53166127]\n",
      "    [-0.54528927]\n",
      "    [-0.55998398]]\n",
      "\n",
      "   [[-0.55228954]\n",
      "    [-0.54762111]\n",
      "    [-0.54354265]\n",
      "    ...\n",
      "    [-0.56735451]\n",
      "    [-0.55361837]\n",
      "    [-0.53964525]]\n",
      "\n",
      "   [[-0.5524492 ]\n",
      "    [-0.55847237]\n",
      "    [-0.56474391]\n",
      "    ...\n",
      "    [-0.57236168]\n",
      "    [-0.56078848]\n",
      "    [-0.54905595]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.60868389]\n",
      "    [-0.59514135]\n",
      "    [-0.58181175]\n",
      "    ...\n",
      "    [-0.56875479]\n",
      "    [-0.55505029]\n",
      "    [-0.54097121]]\n",
      "\n",
      "   [[-0.61036885]\n",
      "    [-0.58156478]\n",
      "    [-0.55345054]\n",
      "    ...\n",
      "    [-0.54359345]\n",
      "    [-0.55259857]\n",
      "    [-0.56214295]]\n",
      "\n",
      "   [[-0.60177584]\n",
      "    [-0.57603537]\n",
      "    [-0.55143087]\n",
      "    ...\n",
      "    [-0.54632169]\n",
      "    [-0.57577664]\n",
      "    [-0.60660701]]]\n",
      "\n",
      "\n",
      "  [[[-0.56542311]\n",
      "    [-0.55523501]\n",
      "    [-0.54572913]\n",
      "    ...\n",
      "    [-0.49422759]\n",
      "    [-0.54073472]\n",
      "    [-0.58994742]]\n",
      "\n",
      "   [[-0.57382855]\n",
      "    [-0.56172652]\n",
      "    [-0.55039784]\n",
      "    ...\n",
      "    [-0.54282377]\n",
      "    [-0.53111771]\n",
      "    [-0.51947378]]\n",
      "\n",
      "   [[-0.56713501]\n",
      "    [-0.5720017 ]\n",
      "    [-0.5768838 ]\n",
      "    ...\n",
      "    [-0.58378627]\n",
      "    [-0.55780937]\n",
      "    [-0.53097433]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56515271]\n",
      "    [-0.56479372]\n",
      "    [-0.56474474]\n",
      "    ...\n",
      "    [-0.56002529]\n",
      "    [-0.57282316]\n",
      "    [-0.58562492]]\n",
      "\n",
      "   [[-0.56908608]\n",
      "    [-0.56977733]\n",
      "    [-0.56995324]\n",
      "    ...\n",
      "    [-0.57776196]\n",
      "    [-0.56565114]\n",
      "    [-0.55219662]]\n",
      "\n",
      "   [[-0.58192878]\n",
      "    [-0.58416324]\n",
      "    [-0.58578737]\n",
      "    ...\n",
      "    [-0.58111976]\n",
      "    [-0.56752885]\n",
      "    [-0.55331752]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-0.58932905]\n",
      "    [-0.55904979]\n",
      "    [-0.5305825 ]\n",
      "    ...\n",
      "    [-0.53218383]\n",
      "    [-0.51854301]\n",
      "    [-0.50569958]]\n",
      "\n",
      "   [[-0.60568522]\n",
      "    [-0.59188944]\n",
      "    [-0.57832632]\n",
      "    ...\n",
      "    [-0.57073093]\n",
      "    [-0.56650663]\n",
      "    [-0.56136316]]\n",
      "\n",
      "   [[-0.57441833]\n",
      "    [-0.55383918]\n",
      "    [-0.53449483]\n",
      "    ...\n",
      "    [-0.55873163]\n",
      "    [-0.56514367]\n",
      "    [-0.57169427]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58065589]\n",
      "    [-0.56348207]\n",
      "    [-0.54722522]\n",
      "    ...\n",
      "    [-0.56350539]\n",
      "    [-0.54927591]\n",
      "    [-0.53522216]]\n",
      "\n",
      "   [[-0.57755187]\n",
      "    [-0.56785932]\n",
      "    [-0.55879805]\n",
      "    ...\n",
      "    [-0.56959982]\n",
      "    [-0.557521  ]\n",
      "    [-0.54522727]]\n",
      "\n",
      "   [[-0.58598271]\n",
      "    [-0.58433619]\n",
      "    [-0.58209614]\n",
      "    ...\n",
      "    [-0.57660542]\n",
      "    [-0.58098682]\n",
      "    [-0.58553015]]]\n",
      "\n",
      "\n",
      "  [[[-0.52195537]\n",
      "    [-0.55532847]\n",
      "    [-0.58655565]\n",
      "    ...\n",
      "    [-0.54982217]\n",
      "    [-0.55968936]\n",
      "    [-0.57048537]]\n",
      "\n",
      "   [[-0.56978244]\n",
      "    [-0.58086167]\n",
      "    [-0.59087523]\n",
      "    ...\n",
      "    [-0.5610617 ]\n",
      "    [-0.57520397]\n",
      "    [-0.5894108 ]]\n",
      "\n",
      "   [[-0.5657004 ]\n",
      "    [-0.56334883]\n",
      "    [-0.56097804]\n",
      "    ...\n",
      "    [-0.56172632]\n",
      "    [-0.56202126]\n",
      "    [-0.56222842]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58000465]\n",
      "    [-0.56657615]\n",
      "    [-0.55416102]\n",
      "    ...\n",
      "    [-0.56234857]\n",
      "    [-0.5741383 ]\n",
      "    [-0.58660636]]\n",
      "\n",
      "   [[-0.53789364]\n",
      "    [-0.54862977]\n",
      "    [-0.55946328]\n",
      "    ...\n",
      "    [-0.58500382]\n",
      "    [-0.58417235]\n",
      "    [-0.58316678]]\n",
      "\n",
      "   [[-0.54080796]\n",
      "    [-0.53869204]\n",
      "    [-0.53768771]\n",
      "    ...\n",
      "    [-0.53334501]\n",
      "    [-0.54759117]\n",
      "    [-0.56296253]]]\n",
      "\n",
      "\n",
      "  [[[-0.58411436]\n",
      "    [-0.54383205]\n",
      "    [-0.5048003 ]\n",
      "    ...\n",
      "    [-0.5397421 ]\n",
      "    [-0.56647098]\n",
      "    [-0.59418407]]\n",
      "\n",
      "   [[-0.58661029]\n",
      "    [-0.56492245]\n",
      "    [-0.54414858]\n",
      "    ...\n",
      "    [-0.56124537]\n",
      "    [-0.56298533]\n",
      "    [-0.56453096]]\n",
      "\n",
      "   [[-0.56897795]\n",
      "    [-0.55459105]\n",
      "    [-0.54031215]\n",
      "    ...\n",
      "    [-0.55012525]\n",
      "    [-0.5634215 ]\n",
      "    [-0.57734745]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57852624]\n",
      "    [-0.57319547]\n",
      "    [-0.56798613]\n",
      "    ...\n",
      "    [-0.55527805]\n",
      "    [-0.56608338]\n",
      "    [-0.57752717]]\n",
      "\n",
      "   [[-0.59208163]\n",
      "    [-0.58577579]\n",
      "    [-0.57970885]\n",
      "    ...\n",
      "    [-0.57975445]\n",
      "    [-0.58135777]\n",
      "    [-0.58307601]]\n",
      "\n",
      "   [[-0.56951701]\n",
      "    [-0.56292968]\n",
      "    [-0.55663064]\n",
      "    ...\n",
      "    [-0.59105351]\n",
      "    [-0.56956817]\n",
      "    [-0.546957  ]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[-0.58664176]\n",
      "    [-0.58495646]\n",
      "    [-0.58347985]\n",
      "    ...\n",
      "    [-0.55220377]\n",
      "    [-0.57280576]\n",
      "    [-0.5945145 ]]\n",
      "\n",
      "   [[-0.56278648]\n",
      "    [-0.57774721]\n",
      "    [-0.59220063]\n",
      "    ...\n",
      "    [-0.57765765]\n",
      "    [-0.58138337]\n",
      "    [-0.58534324]]\n",
      "\n",
      "   [[-0.54665907]\n",
      "    [-0.57254116]\n",
      "    [-0.5972753 ]\n",
      "    ...\n",
      "    [-0.58901639]\n",
      "    [-0.5802736 ]\n",
      "    [-0.57067675]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.54591041]\n",
      "    [-0.55179728]\n",
      "    [-0.55752045]\n",
      "    ...\n",
      "    [-0.54315894]\n",
      "    [-0.55945691]\n",
      "    [-0.57666392]]\n",
      "\n",
      "   [[-0.57421151]\n",
      "    [-0.56328561]\n",
      "    [-0.55339207]\n",
      "    ...\n",
      "    [-0.53663899]\n",
      "    [-0.55236227]\n",
      "    [-0.56892732]]\n",
      "\n",
      "   [[-0.5988953 ]\n",
      "    [-0.60390197]\n",
      "    [-0.60737428]\n",
      "    ...\n",
      "    [-0.59710549]\n",
      "    [-0.57199232]\n",
      "    [-0.54625283]]]\n",
      "\n",
      "\n",
      "  [[[-0.5606449 ]\n",
      "    [-0.56206795]\n",
      "    [-0.56387698]\n",
      "    ...\n",
      "    [-0.56116183]\n",
      "    [-0.57555716]\n",
      "    [-0.59043851]]\n",
      "\n",
      "   [[-0.57020025]\n",
      "    [-0.55714655]\n",
      "    [-0.54514492]\n",
      "    ...\n",
      "    [-0.55332908]\n",
      "    [-0.57102614]\n",
      "    [-0.58924491]]\n",
      "\n",
      "   [[-0.55712881]\n",
      "    [-0.55358833]\n",
      "    [-0.55087129]\n",
      "    ...\n",
      "    [-0.5595547 ]\n",
      "    [-0.55584163]\n",
      "    [-0.55219565]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.54779568]\n",
      "    [-0.55128621]\n",
      "    [-0.55455557]\n",
      "    ...\n",
      "    [-0.58174161]\n",
      "    [-0.57796498]\n",
      "    [-0.57401543]]\n",
      "\n",
      "   [[-0.56746564]\n",
      "    [-0.55250564]\n",
      "    [-0.53853307]\n",
      "    ...\n",
      "    [-0.56876786]\n",
      "    [-0.57649237]\n",
      "    [-0.58460527]]\n",
      "\n",
      "   [[-0.59228197]\n",
      "    [-0.57025899]\n",
      "    [-0.54946584]\n",
      "    ...\n",
      "    [-0.52164469]\n",
      "    [-0.54295088]\n",
      "    [-0.56638167]]]\n",
      "\n",
      "\n",
      "  [[[-0.58235152]\n",
      "    [-0.57490957]\n",
      "    [-0.56688632]\n",
      "    ...\n",
      "    [-0.4982661 ]\n",
      "    [-0.54538948]\n",
      "    [-0.59536298]]\n",
      "\n",
      "   [[-0.58776983]\n",
      "    [-0.56656204]\n",
      "    [-0.54649074]\n",
      "    ...\n",
      "    [-0.55329991]\n",
      "    [-0.55805207]\n",
      "    [-0.56341245]]\n",
      "\n",
      "   [[-0.58652804]\n",
      "    [-0.57617542]\n",
      "    [-0.56573753]\n",
      "    ...\n",
      "    [-0.54603673]\n",
      "    [-0.53153097]\n",
      "    [-0.51676686]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.52741911]\n",
      "    [-0.56761635]\n",
      "    [-0.60559645]\n",
      "    ...\n",
      "    [-0.5420644 ]\n",
      "    [-0.5463571 ]\n",
      "    [-0.55163258]]\n",
      "\n",
      "   [[-0.5794373 ]\n",
      "    [-0.59589637]\n",
      "    [-0.61135692]\n",
      "    ...\n",
      "    [-0.57078602]\n",
      "    [-0.57052884]\n",
      "    [-0.57018025]]\n",
      "\n",
      "   [[-0.58254508]\n",
      "    [-0.58335048]\n",
      "    [-0.58381535]\n",
      "    ...\n",
      "    [-0.58674107]\n",
      "    [-0.57730797]\n",
      "    [-0.56685706]]]]\n",
      "\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      "\n",
      " [[[[-0.56275512]\n",
      "    [-0.58248795]\n",
      "    [-0.60096976]\n",
      "    ...\n",
      "    [-0.58183244]\n",
      "    [-0.58961174]\n",
      "    [-0.59752989]]\n",
      "\n",
      "   [[-0.60444484]\n",
      "    [-0.58524309]\n",
      "    [-0.56681239]\n",
      "    ...\n",
      "    [-0.5769964 ]\n",
      "    [-0.56853818]\n",
      "    [-0.55975359]]\n",
      "\n",
      "   [[-0.59630833]\n",
      "    [-0.56588784]\n",
      "    [-0.53717379]\n",
      "    ...\n",
      "    [-0.54817698]\n",
      "    [-0.56263331]\n",
      "    [-0.57736723]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.59632358]\n",
      "    [-0.58825493]\n",
      "    [-0.58057051]\n",
      "    ...\n",
      "    [-0.56585542]\n",
      "    [-0.55721822]\n",
      "    [-0.54822219]]\n",
      "\n",
      "   [[-0.57833515]\n",
      "    [-0.5754682 ]\n",
      "    [-0.57269962]\n",
      "    ...\n",
      "    [-0.58151132]\n",
      "    [-0.57353074]\n",
      "    [-0.5654146 ]]\n",
      "\n",
      "   [[-0.6058118 ]\n",
      "    [-0.58809059]\n",
      "    [-0.57119826]\n",
      "    ...\n",
      "    [-0.56689481]\n",
      "    [-0.57355195]\n",
      "    [-0.58121234]]]\n",
      "\n",
      "\n",
      "  [[[-0.54840956]\n",
      "    [-0.53912915]\n",
      "    [-0.53077004]\n",
      "    ...\n",
      "    [-0.55503368]\n",
      "    [-0.57052268]\n",
      "    [-0.58676852]]\n",
      "\n",
      "   [[-0.57261919]\n",
      "    [-0.55724754]\n",
      "    [-0.54270817]\n",
      "    ...\n",
      "    [-0.55351046]\n",
      "    [-0.56785991]\n",
      "    [-0.58301736]]\n",
      "\n",
      "   [[-0.55998014]\n",
      "    [-0.55771463]\n",
      "    [-0.55566523]\n",
      "    ...\n",
      "    [-0.57514846]\n",
      "    [-0.57491258]\n",
      "    [-0.57444065]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58745268]\n",
      "    [-0.59203528]\n",
      "    [-0.59613336]\n",
      "    ...\n",
      "    [-0.54813464]\n",
      "    [-0.5645825 ]\n",
      "    [-0.58178057]]\n",
      "\n",
      "   [[-0.57988387]\n",
      "    [-0.58312655]\n",
      "    [-0.58572126]\n",
      "    ...\n",
      "    [-0.54489749]\n",
      "    [-0.55151379]\n",
      "    [-0.55841461]]\n",
      "\n",
      "   [[-0.59480992]\n",
      "    [-0.60420389]\n",
      "    [-0.61306894]\n",
      "    ...\n",
      "    [-0.5513077 ]\n",
      "    [-0.56546889]\n",
      "    [-0.58035007]]]\n",
      "\n",
      "\n",
      "  [[[-0.55179974]\n",
      "    [-0.57084938]\n",
      "    [-0.58849568]\n",
      "    ...\n",
      "    [-0.58073131]\n",
      "    [-0.58171386]\n",
      "    [-0.58279463]]\n",
      "\n",
      "   [[-0.57758889]\n",
      "    [-0.56765662]\n",
      "    [-0.55868156]\n",
      "    ...\n",
      "    [-0.5747852 ]\n",
      "    [-0.55602024]\n",
      "    [-0.53668338]]\n",
      "\n",
      "   [[-0.57209804]\n",
      "    [-0.56912163]\n",
      "    [-0.56663941]\n",
      "    ...\n",
      "    [-0.57841393]\n",
      "    [-0.5690458 ]\n",
      "    [-0.55895109]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56798347]\n",
      "    [-0.58346416]\n",
      "    [-0.5982563 ]\n",
      "    ...\n",
      "    [-0.60608356]\n",
      "    [-0.58703326]\n",
      "    [-0.56703622]]\n",
      "\n",
      "   [[-0.60165056]\n",
      "    [-0.589238  ]\n",
      "    [-0.57720015]\n",
      "    ...\n",
      "    [-0.53057101]\n",
      "    [-0.53550363]\n",
      "    [-0.54133209]]\n",
      "\n",
      "   [[-0.54781786]\n",
      "    [-0.52317778]\n",
      "    [-0.50059595]\n",
      "    ...\n",
      "    [-0.57860172]\n",
      "    [-0.58528726]\n",
      "    [-0.59178679]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[-0.59474212]\n",
      "    [-0.5742613 ]\n",
      "    [-0.55515301]\n",
      "    ...\n",
      "    [-0.54631622]\n",
      "    [-0.56529813]\n",
      "    [-0.58583723]]\n",
      "\n",
      "   [[-0.57894793]\n",
      "    [-0.58474994]\n",
      "    [-0.5895722 ]\n",
      "    ...\n",
      "    [-0.58910592]\n",
      "    [-0.58113122]\n",
      "    [-0.57257438]]\n",
      "\n",
      "   [[-0.58277814]\n",
      "    [-0.56892425]\n",
      "    [-0.55538815]\n",
      "    ...\n",
      "    [-0.59148068]\n",
      "    [-0.58148651]\n",
      "    [-0.57047539]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56958397]\n",
      "    [-0.57474458]\n",
      "    [-0.57980755]\n",
      "    ...\n",
      "    [-0.53976674]\n",
      "    [-0.55116276]\n",
      "    [-0.56300179]]\n",
      "\n",
      "   [[-0.5565366 ]\n",
      "    [-0.56998951]\n",
      "    [-0.58285458]\n",
      "    ...\n",
      "    [-0.56800532]\n",
      "    [-0.57442969]\n",
      "    [-0.58096219]]\n",
      "\n",
      "   [[-0.57678966]\n",
      "    [-0.56988481]\n",
      "    [-0.56326155]\n",
      "    ...\n",
      "    [-0.59958083]\n",
      "    [-0.60033653]\n",
      "    [-0.60062439]]]\n",
      "\n",
      "\n",
      "  [[[-0.57650865]\n",
      "    [-0.56796192]\n",
      "    [-0.56042453]\n",
      "    ...\n",
      "    [-0.56938467]\n",
      "    [-0.5712083 ]\n",
      "    [-0.57342634]]\n",
      "\n",
      "   [[-0.57642656]\n",
      "    [-0.57027221]\n",
      "    [-0.56412587]\n",
      "    ...\n",
      "    [-0.58443416]\n",
      "    [-0.58629782]\n",
      "    [-0.58790519]]\n",
      "\n",
      "   [[-0.59009735]\n",
      "    [-0.57659506]\n",
      "    [-0.56391159]\n",
      "    ...\n",
      "    [-0.56054527]\n",
      "    [-0.56564249]\n",
      "    [-0.57088797]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.52491744]\n",
      "    [-0.5504332 ]\n",
      "    [-0.57483891]\n",
      "    ...\n",
      "    [-0.51873066]\n",
      "    [-0.54791956]\n",
      "    [-0.57885907]]\n",
      "\n",
      "   [[-0.57637008]\n",
      "    [-0.57282756]\n",
      "    [-0.56913288]\n",
      "    ...\n",
      "    [-0.55575197]\n",
      "    [-0.56909708]\n",
      "    [-0.58355667]]\n",
      "\n",
      "   [[-0.58523822]\n",
      "    [-0.58548003]\n",
      "    [-0.5852019 ]\n",
      "    ...\n",
      "    [-0.58060642]\n",
      "    [-0.56920878]\n",
      "    [-0.55723206]]]\n",
      "\n",
      "\n",
      "  [[[-0.60291219]\n",
      "    [-0.57800992]\n",
      "    [-0.55402278]\n",
      "    ...\n",
      "    [-0.58737875]\n",
      "    [-0.59536677]\n",
      "    [-0.6038071 ]]\n",
      "\n",
      "   [[-0.53597002]\n",
      "    [-0.56892846]\n",
      "    [-0.60036421]\n",
      "    ...\n",
      "    [-0.56965124]\n",
      "    [-0.57215127]\n",
      "    [-0.57473994]]\n",
      "\n",
      "   [[-0.56007572]\n",
      "    [-0.56666021]\n",
      "    [-0.57255628]\n",
      "    ...\n",
      "    [-0.57195777]\n",
      "    [-0.56526148]\n",
      "    [-0.55860802]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58067669]\n",
      "    [-0.57725441]\n",
      "    [-0.57361299]\n",
      "    ...\n",
      "    [-0.57085481]\n",
      "    [-0.56985637]\n",
      "    [-0.56874423]]\n",
      "\n",
      "   [[-0.56649288]\n",
      "    [-0.56041421]\n",
      "    [-0.55466966]\n",
      "    ...\n",
      "    [-0.56314708]\n",
      "    [-0.58083011]\n",
      "    [-0.59885632]]\n",
      "\n",
      "   [[-0.58643533]\n",
      "    [-0.56350347]\n",
      "    [-0.54225673]\n",
      "    ...\n",
      "    [-0.51635848]\n",
      "    [-0.54107858]\n",
      "    [-0.56817168]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-0.54527286]\n",
      "    [-0.55697642]\n",
      "    [-0.56856959]\n",
      "    ...\n",
      "    [-0.5841724 ]\n",
      "    [-0.56349415]\n",
      "    [-0.5419775 ]]\n",
      "\n",
      "   [[-0.55629568]\n",
      "    [-0.56214481]\n",
      "    [-0.56750534]\n",
      "    ...\n",
      "    [-0.6012423 ]\n",
      "    [-0.57735472]\n",
      "    [-0.55226602]]\n",
      "\n",
      "   [[-0.56507976]\n",
      "    [-0.5712118 ]\n",
      "    [-0.57638953]\n",
      "    ...\n",
      "    [-0.55755383]\n",
      "    [-0.58282238]\n",
      "    [-0.60946714]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.55445592]\n",
      "    [-0.55288743]\n",
      "    [-0.55186939]\n",
      "    ...\n",
      "    [-0.58512094]\n",
      "    [-0.58562508]\n",
      "    [-0.58580944]]\n",
      "\n",
      "   [[-0.54252896]\n",
      "    [-0.56705953]\n",
      "    [-0.59061411]\n",
      "    ...\n",
      "    [-0.59809364]\n",
      "    [-0.58177586]\n",
      "    [-0.56453253]]\n",
      "\n",
      "   [[-0.60309048]\n",
      "    [-0.58573202]\n",
      "    [-0.56786719]\n",
      "    ...\n",
      "    [-0.60697079]\n",
      "    [-0.60311074]\n",
      "    [-0.59814257]]]\n",
      "\n",
      "\n",
      "  [[[-0.53890295]\n",
      "    [-0.57824993]\n",
      "    [-0.61545966]\n",
      "    ...\n",
      "    [-0.58511764]\n",
      "    [-0.56859836]\n",
      "    [-0.55160328]]\n",
      "\n",
      "   [[-0.55394363]\n",
      "    [-0.58059897]\n",
      "    [-0.60577814]\n",
      "    ...\n",
      "    [-0.56585604]\n",
      "    [-0.54703315]\n",
      "    [-0.52797254]]\n",
      "\n",
      "   [[-0.55235127]\n",
      "    [-0.56326699]\n",
      "    [-0.57379329]\n",
      "    ...\n",
      "    [-0.56683654]\n",
      "    [-0.57127167]\n",
      "    [-0.57581463]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56566323]\n",
      "    [-0.57183325]\n",
      "    [-0.57757889]\n",
      "    ...\n",
      "    [-0.60669791]\n",
      "    [-0.60179051]\n",
      "    [-0.59617168]]\n",
      "\n",
      "   [[-0.55887361]\n",
      "    [-0.55259987]\n",
      "    [-0.54697974]\n",
      "    ...\n",
      "    [-0.57293015]\n",
      "    [-0.56965351]\n",
      "    [-0.56615347]]\n",
      "\n",
      "   [[-0.57594913]\n",
      "    [-0.58105972]\n",
      "    [-0.58497222]\n",
      "    ...\n",
      "    [-0.60270571]\n",
      "    [-0.59643554]\n",
      "    [-0.58931644]]]\n",
      "\n",
      "\n",
      "  [[[-0.60299892]\n",
      "    [-0.56281103]\n",
      "    [-0.52472334]\n",
      "    ...\n",
      "    [-0.58689106]\n",
      "    [-0.58567682]\n",
      "    [-0.58424132]]\n",
      "\n",
      "   [[-0.57997349]\n",
      "    [-0.55414201]\n",
      "    [-0.52927362]\n",
      "    ...\n",
      "    [-0.54510846]\n",
      "    [-0.570301  ]\n",
      "    [-0.59679849]]\n",
      "\n",
      "   [[-0.55653235]\n",
      "    [-0.56871237]\n",
      "    [-0.58018441]\n",
      "    ...\n",
      "    [-0.57897411]\n",
      "    [-0.59270045]\n",
      "    [-0.60678603]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57854215]\n",
      "    [-0.57229226]\n",
      "    [-0.56609802]\n",
      "    ...\n",
      "    [-0.58318208]\n",
      "    [-0.58874362]\n",
      "    [-0.59415166]]\n",
      "\n",
      "   [[-0.60779113]\n",
      "    [-0.57425177]\n",
      "    [-0.54257187]\n",
      "    ...\n",
      "    [-0.55772619]\n",
      "    [-0.58079157]\n",
      "    [-0.60513256]]\n",
      "\n",
      "   [[-0.54035537]\n",
      "    [-0.53839276]\n",
      "    [-0.53683193]\n",
      "    ...\n",
      "    [-0.48906463]\n",
      "    [-0.54067007]\n",
      "    [-0.59502844]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[-0.53816838]\n",
      "    [-0.55700102]\n",
      "    [-0.57506678]\n",
      "    ...\n",
      "    [-0.56596105]\n",
      "    [-0.55939684]\n",
      "    [-0.55215928]]\n",
      "\n",
      "   [[-0.5737251 ]\n",
      "    [-0.55494253]\n",
      "    [-0.53730342]\n",
      "    ...\n",
      "    [-0.56823326]\n",
      "    [-0.57520265]\n",
      "    [-0.58241406]]\n",
      "\n",
      "   [[-0.54682753]\n",
      "    [-0.55919257]\n",
      "    [-0.57098112]\n",
      "    ...\n",
      "    [-0.55748209]\n",
      "    [-0.56506619]\n",
      "    [-0.57308404]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57504134]\n",
      "    [-0.56612809]\n",
      "    [-0.55786123]\n",
      "    ...\n",
      "    [-0.5632725 ]\n",
      "    [-0.57594737]\n",
      "    [-0.58942328]]\n",
      "\n",
      "   [[-0.59649089]\n",
      "    [-0.59526321]\n",
      "    [-0.59372296]\n",
      "    ...\n",
      "    [-0.58023686]\n",
      "    [-0.59392038]\n",
      "    [-0.60846505]]\n",
      "\n",
      "   [[-0.58110944]\n",
      "    [-0.5972263 ]\n",
      "    [-0.61205214]\n",
      "    ...\n",
      "    [-0.54954507]\n",
      "    [-0.57006999]\n",
      "    [-0.59133366]]]\n",
      "\n",
      "\n",
      "  [[[-0.60529235]\n",
      "    [-0.5665563 ]\n",
      "    [-0.52992889]\n",
      "    ...\n",
      "    [-0.60564213]\n",
      "    [-0.59646754]\n",
      "    [-0.5865276 ]]\n",
      "\n",
      "   [[-0.60789765]\n",
      "    [-0.59104242]\n",
      "    [-0.57475172]\n",
      "    ...\n",
      "    [-0.57908315]\n",
      "    [-0.57073285]\n",
      "    [-0.56178052]]\n",
      "\n",
      "   [[-0.57811627]\n",
      "    [-0.58196148]\n",
      "    [-0.58573052]\n",
      "    ...\n",
      "    [-0.57542858]\n",
      "    [-0.5715399 ]\n",
      "    [-0.56748835]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.55814574]\n",
      "    [-0.56902862]\n",
      "    [-0.5792493 ]\n",
      "    ...\n",
      "    [-0.58699925]\n",
      "    [-0.57018934]\n",
      "    [-0.55299571]]\n",
      "\n",
      "   [[-0.53550182]\n",
      "    [-0.5543353 ]\n",
      "    [-0.57256771]\n",
      "    ...\n",
      "    [-0.58855853]\n",
      "    [-0.57545266]\n",
      "    [-0.56181451]]\n",
      "\n",
      "   [[-0.54664472]\n",
      "    [-0.57456678]\n",
      "    [-0.60160686]\n",
      "    ...\n",
      "    [-0.57144561]\n",
      "    [-0.59218467]\n",
      "    [-0.61437224]]]\n",
      "\n",
      "\n",
      "  [[[-0.52687426]\n",
      "    [-0.55917071]\n",
      "    [-0.58977592]\n",
      "    ...\n",
      "    [-0.59444515]\n",
      "    [-0.60313996]\n",
      "    [-0.6121605 ]]\n",
      "\n",
      "   [[-0.56592505]\n",
      "    [-0.55440412]\n",
      "    [-0.54397216]\n",
      "    ...\n",
      "    [-0.56200325]\n",
      "    [-0.56497195]\n",
      "    [-0.56777418]]\n",
      "\n",
      "   [[-0.55486294]\n",
      "    [-0.57203346]\n",
      "    [-0.58819995]\n",
      "    ...\n",
      "    [-0.57797307]\n",
      "    [-0.55452028]\n",
      "    [-0.53039721]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.54834859]\n",
      "    [-0.56222635]\n",
      "    [-0.57523892]\n",
      "    ...\n",
      "    [-0.59100807]\n",
      "    [-0.59408528]\n",
      "    [-0.59668955]]\n",
      "\n",
      "   [[-0.5721792 ]\n",
      "    [-0.55300712]\n",
      "    [-0.53510051]\n",
      "    ...\n",
      "    [-0.58634135]\n",
      "    [-0.59463322]\n",
      "    [-0.60288892]]\n",
      "\n",
      "   [[-0.5851058 ]\n",
      "    [-0.54891457]\n",
      "    [-0.51471391]\n",
      "    ...\n",
      "    [-0.57840148]\n",
      "    [-0.58061208]\n",
      "    [-0.58270937]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-0.60938823]\n",
      "    [-0.60452825]\n",
      "    [-0.59965012]\n",
      "    ...\n",
      "    [-0.60533576]\n",
      "    [-0.5806246 ]\n",
      "    [-0.55432463]]\n",
      "\n",
      "   [[-0.59425924]\n",
      "    [-0.58760794]\n",
      "    [-0.58098009]\n",
      "    ...\n",
      "    [-0.56766732]\n",
      "    [-0.56304368]\n",
      "    [-0.55819267]]\n",
      "\n",
      "   [[-0.57947148]\n",
      "    [-0.57611537]\n",
      "    [-0.57270518]\n",
      "    ...\n",
      "    [-0.56190577]\n",
      "    [-0.5634939 ]\n",
      "    [-0.5656226 ]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56353242]\n",
      "    [-0.57255504]\n",
      "    [-0.5810856 ]\n",
      "    ...\n",
      "    [-0.59975148]\n",
      "    [-0.57557411]\n",
      "    [-0.55042574]]\n",
      "\n",
      "   [[-0.56041321]\n",
      "    [-0.56205944]\n",
      "    [-0.56412779]\n",
      "    ...\n",
      "    [-0.58917316]\n",
      "    [-0.57888356]\n",
      "    [-0.56795515]]\n",
      "\n",
      "   [[-0.59729138]\n",
      "    [-0.59227988]\n",
      "    [-0.58758724]\n",
      "    ...\n",
      "    [-0.56670152]\n",
      "    [-0.56550103]\n",
      "    [-0.56463513]]]\n",
      "\n",
      "\n",
      "  [[[-0.59158502]\n",
      "    [-0.5698553 ]\n",
      "    [-0.54966652]\n",
      "    ...\n",
      "    [-0.53911883]\n",
      "    [-0.56298122]\n",
      "    [-0.58857019]]\n",
      "\n",
      "   [[-0.59789747]\n",
      "    [-0.58694869]\n",
      "    [-0.57664521]\n",
      "    ...\n",
      "    [-0.5830683 ]\n",
      "    [-0.58290153]\n",
      "    [-0.58248328]]\n",
      "\n",
      "   [[-0.5621974 ]\n",
      "    [-0.5758091 ]\n",
      "    [-0.58850586]\n",
      "    ...\n",
      "    [-0.56153818]\n",
      "    [-0.56751473]\n",
      "    [-0.57375075]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58044586]\n",
      "    [-0.57367405]\n",
      "    [-0.56693462]\n",
      "    ...\n",
      "    [-0.60440119]\n",
      "    [-0.59346418]\n",
      "    [-0.58191408]]\n",
      "\n",
      "   [[-0.54770164]\n",
      "    [-0.5547548 ]\n",
      "    [-0.56183692]\n",
      "    ...\n",
      "    [-0.5888194 ]\n",
      "    [-0.58233561]\n",
      "    [-0.575869  ]]\n",
      "\n",
      "   [[-0.57090271]\n",
      "    [-0.56847297]\n",
      "    [-0.5658456 ]\n",
      "    ...\n",
      "    [-0.57038324]\n",
      "    [-0.58946726]\n",
      "    [-0.60895361]]]\n",
      "\n",
      "\n",
      "  [[[-0.59934602]\n",
      "    [-0.56503136]\n",
      "    [-0.53263745]\n",
      "    ...\n",
      "    [-0.59237693]\n",
      "    [-0.59376524]\n",
      "    [-0.59476345]]\n",
      "\n",
      "   [[-0.57347284]\n",
      "    [-0.55298989]\n",
      "    [-0.53381836]\n",
      "    ...\n",
      "    [-0.59119406]\n",
      "    [-0.57725482]\n",
      "    [-0.56247193]]\n",
      "\n",
      "   [[-0.58030737]\n",
      "    [-0.58346737]\n",
      "    [-0.58658174]\n",
      "    ...\n",
      "    [-0.57885924]\n",
      "    [-0.56265514]\n",
      "    [-0.54617765]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.58273503]\n",
      "    [-0.57806465]\n",
      "    [-0.57392227]\n",
      "    ...\n",
      "    [-0.57253537]\n",
      "    [-0.56881444]\n",
      "    [-0.56537949]]\n",
      "\n",
      "   [[-0.59172631]\n",
      "    [-0.58748871]\n",
      "    [-0.58345463]\n",
      "    ...\n",
      "    [-0.56347908]\n",
      "    [-0.5678163 ]\n",
      "    [-0.57235756]]\n",
      "\n",
      "   [[-0.5659181 ]\n",
      "    [-0.55472521]\n",
      "    [-0.54434313]\n",
      "    ...\n",
      "    [-0.56167199]\n",
      "    [-0.57490112]\n",
      "    [-0.58884292]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[-0.59049609]\n",
      "    [-0.58397052]\n",
      "    [-0.57773701]\n",
      "    ...\n",
      "    [-0.60451189]\n",
      "    [-0.56470937]\n",
      "    [-0.52306739]]\n",
      "\n",
      "   [[-0.60367613]\n",
      "    [-0.58084766]\n",
      "    [-0.55819631]\n",
      "    ...\n",
      "    [-0.5899318 ]\n",
      "    [-0.58525275]\n",
      "    [-0.58036482]]\n",
      "\n",
      "   [[-0.59187234]\n",
      "    [-0.58015427]\n",
      "    [-0.56828336]\n",
      "    ...\n",
      "    [-0.58996099]\n",
      "    [-0.58967676]\n",
      "    [-0.58956541]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.56838691]\n",
      "    [-0.56890988]\n",
      "    [-0.56926955]\n",
      "    ...\n",
      "    [-0.60410483]\n",
      "    [-0.57889563]\n",
      "    [-0.55201553]]\n",
      "\n",
      "   [[-0.56498008]\n",
      "    [-0.5781164 ]\n",
      "    [-0.59033805]\n",
      "    ...\n",
      "    [-0.5830118 ]\n",
      "    [-0.58131873]\n",
      "    [-0.57954713]]\n",
      "\n",
      "   [[-0.58726947]\n",
      "    [-0.57246348]\n",
      "    [-0.55822238]\n",
      "    ...\n",
      "    [-0.54920947]\n",
      "    [-0.56942401]\n",
      "    [-0.59081739]]]\n",
      "\n",
      "\n",
      "  [[[-0.58256522]\n",
      "    [-0.56839318]\n",
      "    [-0.55562884]\n",
      "    ...\n",
      "    [-0.59934468]\n",
      "    [-0.60143684]\n",
      "    [-0.6033274 ]]\n",
      "\n",
      "   [[-0.52891553]\n",
      "    [-0.55265286]\n",
      "    [-0.57602444]\n",
      "    ...\n",
      "    [-0.56626707]\n",
      "    [-0.56078662]\n",
      "    [-0.55494348]]\n",
      "\n",
      "   [[-0.55837661]\n",
      "    [-0.57046304]\n",
      "    [-0.58233994]\n",
      "    ...\n",
      "    [-0.56928613]\n",
      "    [-0.54700162]\n",
      "    [-0.52428561]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.57416299]\n",
      "    [-0.57057684]\n",
      "    [-0.56733202]\n",
      "    ...\n",
      "    [-0.5423617 ]\n",
      "    [-0.55442781]\n",
      "    [-0.56760358]]\n",
      "\n",
      "   [[-0.57931702]\n",
      "    [-0.58149982]\n",
      "    [-0.58337448]\n",
      "    ...\n",
      "    [-0.57872627]\n",
      "    [-0.57494099]\n",
      "    [-0.57097088]]\n",
      "\n",
      "   [[-0.56383148]\n",
      "    [-0.56746539]\n",
      "    [-0.57059375]\n",
      "    ...\n",
      "    [-0.57436945]\n",
      "    [-0.57271083]\n",
      "    [-0.57073276]]]\n",
      "\n",
      "\n",
      "  [[[-0.58476134]\n",
      "    [-0.57638293]\n",
      "    [-0.56851223]\n",
      "    ...\n",
      "    [-0.54665851]\n",
      "    [-0.54410084]\n",
      "    [-0.54137441]]\n",
      "\n",
      "   [[-0.5321038 ]\n",
      "    [-0.54987732]\n",
      "    [-0.56698787]\n",
      "    ...\n",
      "    [-0.57816782]\n",
      "    [-0.5835075 ]\n",
      "    [-0.58894825]]\n",
      "\n",
      "   [[-0.55982567]\n",
      "    [-0.56329672]\n",
      "    [-0.56657006]\n",
      "    ...\n",
      "    [-0.58402364]\n",
      "    [-0.59411244]\n",
      "    [-0.60481583]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[-0.55667046]\n",
      "    [-0.57715735]\n",
      "    [-0.5966094 ]\n",
      "    ...\n",
      "    [-0.60441348]\n",
      "    [-0.5877745 ]\n",
      "    [-0.57046693]]\n",
      "\n",
      "   [[-0.56291396]\n",
      "    [-0.56646589]\n",
      "    [-0.5700809 ]\n",
      "    ...\n",
      "    [-0.572815  ]\n",
      "    [-0.56692674]\n",
      "    [-0.5611524 ]]\n",
      "\n",
      "   [[-0.55458652]\n",
      "    [-0.57076778]\n",
      "    [-0.58646458]\n",
      "    ...\n",
      "    [-0.56517926]\n",
      "    [-0.56422639]\n",
      "    [-0.5636827 ]]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60, 64, 64, 64, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(model_path):  \n",
    "    \"\"\"  \n",
    "    加载预训练模型  \n",
    "    \"\"\"  \n",
    "    # 确保模型文件存在  \n",
    "    if not os.path.exists(model_path):  \n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")  \n",
    "    \n",
    "    # 加载完整的检查点  \n",
    "    checkpoint = torch.load(model_path)  \n",
    "    \n",
    "    # 创建模型实例  \n",
    "    model = ImprovedCNNTransformer()  \n",
    "    \n",
    "    # 只加载模型状态字典  \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  \n",
    "    model.eval()  # 设置为评估模式  \n",
    "    \n",
    "    # 如果有GPU则使用GPU  \n",
    "    if torch.cuda.is_available():  \n",
    "        model = model.cuda()  \n",
    "    \n",
    "    return model \n",
    "\n",
    "\n",
    "model_path = \"/root/experiments/20250117_141323/checkpoint_epoch_64.pth\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "data_path = \"/root/autodl-tmp/CNNLSTM/Project/preprocssed_60_64/sub-0010001_ses-1_task-rest_run-1_bold.nii.gz.npy\" \n",
    "data_npy = np.load(data_path)\n",
    "print(data_npy)\n",
    "data_npy.shape\n",
    "# out_put = model(data_npy)\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 60, 1, 64, 64, 64])\n",
      "tensor([[ 0.7346, -1.3534]], device='cuda:0')\n",
      "Output shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_inference(data_npy):  \n",
    "    \"\"\"  \n",
    "    预处理数据用于模型推理  \n",
    "    原始数据形状: (60, 64, 64, 64, 1)  \n",
    "    目标形状: (batch_size, time_steps, 1, 64, 64, 64)  \n",
    "    \"\"\"  \n",
    "    # 转换为torch张量  \n",
    "    data = torch.from_numpy(data_npy)  \n",
    "    \n",
    "    # 确保数据类型为float32  \n",
    "    if data.dtype != torch.float32:  \n",
    "        data = data.float()  \n",
    "    \n",
    "    # 调整维度顺序  \n",
    "    # 从 (60, 64, 64, 64, 1) 转换为 (1, 60, 1, 64, 64, 64)  \n",
    "    data = data.permute(0, 4, 1, 2, 3)  # 现在是 (60, 1, 64, 64, 64)  \n",
    "    data = data.unsqueeze(0)  # 添加batch维度，变成 (1, 60, 1, 64, 64, 64)  \n",
    "    \n",
    "    # 如果模型在GPU上，将数据也移到GPU  \n",
    "    if torch.cuda.is_available():  \n",
    "        data = data.cuda()  \n",
    "    \n",
    "    return data  \n",
    "\n",
    "# 使用示例  \n",
    "data = prepare_data_for_inference(data_npy)  \n",
    "print(f\"Input shape: {data.shape}\")  # 应该输出: torch.Size([1, 60, 1, 64, 64, 64])  \n",
    "\n",
    "# 使用模型进行推理  \n",
    "with torch.no_grad():  \n",
    "    output = model(data)  \n",
    "print(output)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and data prepared\n",
      "Input data shape: torch.Size([1, 60, 1, 64, 64, 64])\n",
      "Starting visualization process...\n",
      "Computing Layer GradCAM...\n",
      "Layer GradCAM computation completed\n",
      "Computing attention weights...\n",
      "Attention weights visualization completed\n",
      "Computing Integrated Gradients...\n",
      "Integrated Gradients computation completed\n",
      "Computing and visualizing SHAP values...\n",
      "Computing SHAP values using KernelExplainer...\n",
      "Flattened data shape: (1, 15728640)\n",
      "KernelExplainer created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SHAP values:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0e0295476745fea33a94a7c819a1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing SHAP values:   0%|          | 0/1 [01:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 589\u001b[0m\n\u001b[1;32m    586\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/autodl-tmp/CNNLSTM/Project/preprocssed_60_64/sub-0010001_ses-1_task-rest_run-1_bold.nii.gz.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \n\u001b[0;32m--> 589\u001b[0m     interpreter \u001b[38;5;241m=\u001b[39m \u001b[43mrun_interpretation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully completed all interpretability analyses\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \n",
      "Cell \u001b[0;32mIn[40], line 573\u001b[0m, in \u001b[0;36mrun_interpretation\u001b[0;34m(model_path, data_path)\u001b[0m\n\u001b[1;32m    570\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m ModelInterpreter(model, data)  \n\u001b[1;32m    572\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting visualization process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m--> 573\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_data\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \n\u001b[1;32m    576\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)  \n",
      "Cell \u001b[0;32mIn[40], line 172\u001b[0m, in \u001b[0;36mModelInterpreter.visualize_results\u001b[0;34m(self, save_dir, original_data)\u001b[0m\n\u001b[1;32m    169\u001b[0m reshaped_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reshape_data_for_shap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)  \n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# 计算SHAP值  \u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_shap_values_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 可以调整采样数量  \u001b[39;49;00m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shap_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:  \n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# 处理多类别情况  \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 360\u001b[0m, in \u001b[0;36mModelInterpreter.compute_shap_values_batch\u001b[0;34m(self, background_data, n_samples, batch_size)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \n\u001b[1;32m    359\u001b[0m     batch \u001b[38;5;241m=\u001b[39m data_flat[i:i \u001b[38;5;241m+\u001b[39m batch_size]  \u001b[38;5;66;03m# 使用展平的数据  \u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     batch_shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# 重塑SHAP值回原始维度  \u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_shap_values, \u001b[38;5;28mlist\u001b[39m):  \n\u001b[1;32m    367\u001b[0m         \u001b[38;5;66;03m# 多类别情况  \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/explainers/_kernel.py:244\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[1;32m    243\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[0;32m--> 244\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    246\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/explainers/_kernel.py:275\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m match_instance_to_data(instance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# find the feature groups we will test. If a feature does not change from its\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# current value then we know it doesn't impact the model\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvarying_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingFeatureGroups \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds])\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/explainers/_kernel.py:477\u001b[0m, in \u001b[0;36mKernelExplainer.varying_groups\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    476\u001b[0m         x_group \u001b[38;5;241m=\u001b[39m x_group\u001b[38;5;241m.\u001b[39mtodense()\n\u001b[0;32m--> 477\u001b[0m     num_mismatches \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrompyfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_equal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    478\u001b[0m     varying[i] \u001b[38;5;241m=\u001b[39m num_mismatches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    479\u001b[0m varying_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(varying)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/explainers/_kernel.py:462\u001b[0m, in \u001b[0;36mKernelExplainer.not_equal\u001b[0;34m(i, j)\u001b[0m\n\u001b[1;32m    460\u001b[0m number_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, number_types) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(j, number_types):\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m j \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36misclose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/numeric.py:2380\u001b[0m, in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2378\u001b[0m yfin \u001b[38;5;241m=\u001b[39m isfinite(y)\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(xfin) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(yfin):\n\u001b[0;32m-> 2380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwithin_tol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2382\u001b[0m     finite \u001b[38;5;241m=\u001b[39m xfin \u001b[38;5;241m&\u001b[39m yfin\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/numpy/core/numeric.py:2361\u001b[0m, in \u001b[0;36misclose.<locals>.within_tol\u001b[0;34m(x, y, atol, rtol)\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithin_tol\u001b[39m(x, y, atol, rtol):\n\u001b[1;32m   2360\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m), _no_nep50_warning():\n\u001b[0;32m-> 2361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m less_equal(\u001b[38;5;28mabs\u001b[39m(x\u001b[38;5;241m-\u001b[39my), atol \u001b[38;5;241m+\u001b[39m rtol \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mabs\u001b[39m(y))\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/contextlib.py:120\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "from captum.attr import IntegratedGradients, LayerGradCam  \n",
    "import os  \n",
    "from typing import Optional, Union, List, Tuple  # 添加这行 \n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  \n",
    "\n",
    "# 在计算密集操作后及时清理显存  \n",
    "torch.cuda.empty_cache()  \n",
    "class ModelInterpreter:  \n",
    "    def __init__(self, model, data):  \n",
    "        self.model = model  \n",
    "        self.data = data  \n",
    "        self.model.eval()  # 确保模型在评估模式  \n",
    "        \n",
    "    def compute_layer_gradcam(self, target_class=1):  \n",
    "        \"\"\"  \n",
    "        使用LayerGradCam替代普通GradCam  \n",
    "        \"\"\"  \n",
    "        # 获取选定的卷积层  \n",
    "        target_layer = self.model.res1  \n",
    "        \n",
    "        # 创建LayerGradCam实例  \n",
    "        layer_gc = LayerGradCam(self.model, target_layer)  \n",
    "        \n",
    "        # 计算属性  \n",
    "        with torch.no_grad():  \n",
    "            # 克隆输入数据以避免修改原始数据  \n",
    "            input_data = self.data.clone()  \n",
    "            \n",
    "        attributions = layer_gc.attribute(input_data, target=target_class)  \n",
    "        return attributions.detach().cpu().numpy()  \n",
    "    \n",
    "    def compute_attention_weights(self):  \n",
    "        \"\"\"  \n",
    "        安全地获取注意力权重  \n",
    "        \"\"\"  \n",
    "        attention_weights = []  \n",
    "        \n",
    "        def hook_fn(module, input, output):  \n",
    "            # 只获取注意力权重，不计算梯度  \n",
    "            with torch.no_grad():  \n",
    "                if hasattr(module, 'self_attn'):  \n",
    "                    # 克隆输入以避免修改  \n",
    "                    q, k, v = [x.clone() for x in input[0:3]]  \n",
    "                    # 计算注意力分数  \n",
    "                    attn_weights = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(k.size(-1))  \n",
    "                    attention_weights.append(attn_weights.cpu().numpy())  \n",
    "        \n",
    "        # 注册钩子  \n",
    "        hooks = []  \n",
    "        for layer in self.model.transformer_encoder.layers:  \n",
    "            hooks.append(layer.self_attn.register_forward_hook(hook_fn))  \n",
    "        \n",
    "        try:  \n",
    "            # 前向传播  \n",
    "            with torch.no_grad():  \n",
    "                _ = self.model(self.data.clone())  \n",
    "        finally:  \n",
    "            # 移除钩子  \n",
    "            for hook in hooks:  \n",
    "                hook.remove()  \n",
    "        \n",
    "        return attention_weights  \n",
    "    \n",
    "    def compute_integrated_gradients(self, target_class=1, steps=20):  # 减少步数  \n",
    "        \"\"\"  \n",
    "        计算Integrated Gradients，使用内存优化  \n",
    "        \"\"\"  \n",
    "        ig = IntegratedGradients(self.model)  \n",
    "        baseline = torch.zeros_like(self.data)  \n",
    "        \n",
    "        # 使用较小的batch处理  \n",
    "        batch_size = 4  # 根据GPU内存调整  \n",
    "        n_batches = steps // batch_size  \n",
    "        attributions_list = []  \n",
    "        \n",
    "        try:  \n",
    "            for i in range(n_batches):  \n",
    "                with torch.no_grad():  \n",
    "                    batch_attributions = ig.attribute(  \n",
    "                        self.data.clone(),  \n",
    "                        baseline,  \n",
    "                        target=target_class,  \n",
    "                        n_steps=batch_size  \n",
    "                    )  \n",
    "                    attributions_list.append(batch_attributions.cpu().numpy())  \n",
    "                \n",
    "                # 清理GPU内存  \n",
    "                torch.cuda.empty_cache()  \n",
    "            \n",
    "            # 合并结果  \n",
    "            attributions = np.concatenate(attributions_list, axis=0)  \n",
    "            attributions = np.mean(attributions, axis=0, keepdims=True)  \n",
    "            \n",
    "            return attributions  \n",
    "        \n",
    "        except Exception as e:  \n",
    "            print(f\"Error in compute_integrated_gradients: {str(e)}\")  \n",
    "            return None    \n",
    "    \n",
    "    def visualize_results(self, save_dir='interpretability_results', original_data=None):  \n",
    "        \"\"\"  \n",
    "        可视化所有结果\n",
    "        \"\"\"  \n",
    "        os.makedirs(save_dir, exist_ok=True)  \n",
    "        \n",
    "        # 预处理原始数据  \n",
    "        if original_data is not None:  \n",
    "            if isinstance(original_data, torch.Tensor):  \n",
    "                original_data = original_data.cpu().numpy()  \n",
    "            if len(original_data.shape) == 6:  # (batch, time, channel, depth, height, width)  \n",
    "                original_data = original_data[0, 0, 0]  # 取第一个样本，第一个时间点，第一个通道  \n",
    "            elif len(original_data.shape) == 5:  # (batch, channel, depth, height, width)  \n",
    "                original_data = original_data[0, 0]  # 取第一个样本，第一个通道  \n",
    "        \n",
    "        # Layer GradCAM可视化  \n",
    "        try:  \n",
    "            print(\"Computing Layer GradCAM...\")  \n",
    "            grad_cam_maps = self.compute_layer_gradcam()  \n",
    "            if grad_cam_maps is not None:  \n",
    "                self._plot_3d_heatmap(  \n",
    "                    grad_cam_maps[0],  \n",
    "                    os.path.join(save_dir, 'layer_gradcam.png'),  \n",
    "                    title='Layer GradCAM Visualization',  \n",
    "                    original_data=original_data  \n",
    "                )  \n",
    "                print(\"Layer GradCAM computation completed\")  \n",
    "        except Exception as e:  \n",
    "            print(f\"Layer GradCAM computation failed: {str(e)}\")  \n",
    "        \n",
    "        # 注意力权重可视化  \n",
    "        try:  \n",
    "            print(\"Computing attention weights...\")  \n",
    "            attention_weights = self.compute_attention_weights()  \n",
    "            if attention_weights:  \n",
    "                for i, attn in enumerate(attention_weights):  \n",
    "                    plt.figure(figsize=(10, 8))  \n",
    "                    sns.heatmap(attn[0], cmap='viridis')  \n",
    "                    plt.title(f'Attention Weights Layer {i+1}')  \n",
    "                    plt.savefig(os.path.join(save_dir, f'attention_layer_{i+1}.png'))  \n",
    "                    plt.close()  \n",
    "            print(\"Attention weights visualization completed\")  \n",
    "        except Exception as e:  \n",
    "            print(f\"Attention visualization failed: {str(e)}\")  \n",
    "        \n",
    "        # Integrated Gradients可视化  \n",
    "        try:  \n",
    "            print(\"Computing Integrated Gradients...\")  \n",
    "            ig_attributions = self.compute_integrated_gradients()  \n",
    "            if ig_attributions is not None:  \n",
    "                self._plot_3d_heatmap(  \n",
    "                    np.mean(ig_attributions[0], axis=0),  \n",
    "                    os.path.join(save_dir, 'integrated_gradients.png'),  \n",
    "                    title='Integrated Gradients',  \n",
    "                    original_data=original_data  \n",
    "                )  \n",
    "                print(\"Integrated Gradients computation completed\")  \n",
    "        except Exception as e:  \n",
    "            print(f\"Integrated Gradients computation failed: {str(e)}\")  \n",
    "    \n",
    "        # SHAP值可视化\n",
    "        # try:  \n",
    "        #     print(\"Computing and visualizing SHAP values...\")  \n",
    "            \n",
    "        #     # 重塑数据以适应SHAP分析  \n",
    "        #     reshaped_data = self._reshape_data_for_shap(self.data)  \n",
    "            \n",
    "        #     # 计算SHAP值  \n",
    "        #     shap_values = self.compute_shap_values_batch(  \n",
    "        #         batch_size=1,  \n",
    "        #         n_samples=20  # 可以调整采样数量  \n",
    "        #     )  \n",
    "            \n",
    "        #     if shap_values is not None:  \n",
    "        #         try:  \n",
    "        #             # 处理多类别情况  \n",
    "        #             if isinstance(shap_values, list):  \n",
    "        #                 shap_values_viz = shap_values[0]  # 使用第一个类别  \n",
    "        #             else:  \n",
    "        #                 shap_values_viz = shap_values  \n",
    "                    \n",
    "        #             # 确保数据类型正确  \n",
    "        #             if isinstance(shap_values_viz, torch.Tensor):  \n",
    "        #                 shap_values_viz = shap_values_viz.cpu().numpy()  \n",
    "                    \n",
    "        #             # 重塑SHAP值回原始维度  \n",
    "        #             if original_data is not None:  \n",
    "        #                 shap_values_viz = shap_values_viz.reshape(original_data.shape)  \n",
    "                    \n",
    "        #             # 绘制SHAP值总结图  \n",
    "        #             self._plot_shap_summary(  \n",
    "        #                 shap_values_viz,  \n",
    "        #                 os.path.join(save_dir, 'shap_summary.png')  \n",
    "        #             )  \n",
    "                    \n",
    "        #             # 绘制SHAP值3D热力图  \n",
    "        #             self._plot_shap_3d_heatmap(  \n",
    "        #                 shap_values_viz,  \n",
    "        #                 os.path.join(save_dir, 'shap_3d_heatmap.png'),  \n",
    "        #                 original_data  \n",
    "        #             )  \n",
    "                    \n",
    "        #             print(\"SHAP visualization completed\")  \n",
    "                    \n",
    "        #         except Exception as e:  \n",
    "        #             print(f\"Error in SHAP visualization: {str(e)}\")  \n",
    "                    \n",
    "        # except Exception as e:  \n",
    "        #     print(f\"SHAP visualization failed: {str(e)}\")  \n",
    "        #     import traceback  \n",
    "        #     traceback.print_exc()  \n",
    "\n",
    "\n",
    "    def _overlay_heatmap(self, original, heatmap, alpha=0.4):  \n",
    "        \"\"\"  \n",
    "        将热力图叠加到原始图像上  \n",
    "        Args:  \n",
    "            original: 原始图像数据  \n",
    "            heatmap: 热力图数据  \n",
    "            alpha: 透明度  \n",
    "        \"\"\"  \n",
    "        # 归一化热力图  \n",
    "        heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))  \n",
    "        \n",
    "        # 使用jet颜色映射转换热力图为RGB  \n",
    "        cmap = plt.cm.jet  \n",
    "        heatmap_colored = cmap(heatmap)[:, :, :3]  # 去掉alpha通道  \n",
    "        \n",
    "        # 归一化原始图像  \n",
    "        original = (original - np.min(original)) / (np.max(original) - np.min(original))  \n",
    "        \n",
    "        # 创建叠加图像  \n",
    "        overlay = original[..., np.newaxis] * (1 - alpha) + heatmap_colored * alpha  \n",
    "        \n",
    "        return overlay  \n",
    "    \n",
    "\n",
    "    def _plot_3d_heatmap(self, data, save_path, title, original_data=None):  \n",
    "        # 确保数据是3D的  \n",
    "        if len(data.shape) > 3:  \n",
    "            data = np.mean(data, axis=0)  \n",
    "        \n",
    "        # 确保数据形状正确  \n",
    "        if original_data is not None:  \n",
    "            # 如果原始数据是5D的 (batch, channel, depth, height, width)，取第一个样本的第一个通道  \n",
    "            if len(original_data.shape) == 5:  \n",
    "                original_data = original_data[0, 0]  \n",
    "            # 如果是6D的 (batch, time, channel, depth, height, width)，取第一个样本，第一个时间点的第一个通道  \n",
    "            elif len(original_data.shape) == 6:  \n",
    "                original_data = original_data[0, 0, 0]  \n",
    "        \n",
    "        # 确保热力图数据大小与原始数据匹配  \n",
    "        if data.shape != original_data.shape:  \n",
    "            # 使用插值调整热力图大小  \n",
    "            from scipy.ndimage import zoom  \n",
    "            zoom_factors = tuple(o/d for o, d in zip(original_data.shape, data.shape))  \n",
    "            data = zoom(data, zoom_factors)  \n",
    "        \n",
    "        # 获取中间切片  \n",
    "        mid_z = data.shape[-1] // 2  \n",
    "        mid_y = data.shape[-2] // 2  \n",
    "        mid_x = data.shape[-3] // 2  \n",
    "          \n",
    "        fig = plt.figure(figsize=(20, 15))  \n",
    "        gs = plt.GridSpec(2, 3)  \n",
    "        \n",
    "        # 1. 原始热力图视图  \n",
    "        ax1 = fig.add_subplot(gs[0, 0])  \n",
    "        ax2 = fig.add_subplot(gs[0, 1])  \n",
    "        ax3 = fig.add_subplot(gs[0, 2])  \n",
    "        \n",
    "        # 绘制热力图  \n",
    "        im1 = ax1.imshow(data[mid_x, :, :], cmap='hot')  \n",
    "        im2 = ax2.imshow(data[:, mid_y, :], cmap='hot')  \n",
    "        im3 = ax3.imshow(data[:, :, mid_z], cmap='hot')  \n",
    "        \n",
    "        ax1.set_title('Sagittal View - Heatmap')  \n",
    "        ax2.set_title('Coronal View - Heatmap')  \n",
    "        ax3.set_title('Axial View - Heatmap')  \n",
    "        \n",
    "        plt.colorbar(im1, ax=ax1)  \n",
    "        plt.colorbar(im2, ax=ax2)  \n",
    "        plt.colorbar(im3, ax=ax3)  \n",
    "        \n",
    "        # 绘制叠加图  \n",
    "        if original_data is not None:  \n",
    "            ax4 = fig.add_subplot(gs[1, 0])  \n",
    "            ax5 = fig.add_subplot(gs[1, 1])  \n",
    "            ax6 = fig.add_subplot(gs[1, 2])  \n",
    "            \n",
    "            # 创建叠加图  \n",
    "            overlay_sagittal = self._overlay_heatmap(  \n",
    "                original_data[mid_x, :, :],  \n",
    "                data[mid_x, :, :]  \n",
    "            )  \n",
    "            overlay_coronal = self._overlay_heatmap(  \n",
    "                original_data[:, mid_y, :],  \n",
    "                data[:, mid_y, :]  \n",
    "            )  \n",
    "            overlay_axial = self._overlay_heatmap(  \n",
    "                original_data[:, :, mid_z],  \n",
    "                data[:, :, mid_z]  \n",
    "            )  \n",
    "            \n",
    "            # 显示叠加图  \n",
    "            ax4.imshow(overlay_sagittal)  \n",
    "            ax5.imshow(overlay_coronal)  \n",
    "            ax6.imshow(overlay_axial)  \n",
    "            \n",
    "            ax4.set_title('Sagittal View - Overlay')  \n",
    "            ax5.set_title('Coronal View - Overlay')  \n",
    "            ax6.set_title('Axial View - Overlay')  \n",
    "        \n",
    "        plt.suptitle(title)  \n",
    "        plt.tight_layout()  \n",
    "        plt.savefig(save_path)  \n",
    "        plt.close()\n",
    "\n",
    "def run_interpretation(model_path, data_path):  \n",
    "    \"\"\"  \n",
    "    运行解释性分析的主函数  \n",
    "    \"\"\"  \n",
    "    # 加载模型和数据  \n",
    "    model = load_model(model_path)  \n",
    "    data_npy = np.load(data_path)  \n",
    "    data = prepare_data_for_inference(data_npy)  \n",
    "    \n",
    "    # 保存原始数据用于可视化  \n",
    "    original_data = data_npy.copy()  \n",
    "    \n",
    "    print(\"Model loaded and data prepared\")  \n",
    "    print(f\"Input data shape: {data.shape}\")  \n",
    "     \n",
    "    interpreter = ModelInterpreter(model, data)  \n",
    "     \n",
    "    print(\"Starting visualization process...\")  \n",
    "    interpreter.visualize_results(original_data=original_data)  \n",
    "     \n",
    "    with torch.no_grad():  \n",
    "        output = model(data)  \n",
    "        predictions = torch.softmax(output, dim=1)  \n",
    "        print(f\"Model predictions: {predictions}\")  \n",
    "    \n",
    "    print(\"Interpretation completed\")  \n",
    "    return interpreter\n",
    "\n",
    "# 使用示例  \n",
    "if __name__ == \"__main__\":  \n",
    "    model_path = \"/root/experiments/20250117_141323/checkpoint_epoch_64.pth\"  \n",
    "    data_path = \"/root/autodl-tmp/CNNLSTM/Project/preprocssed_60_64/sub-0010001_ses-1_task-rest_run-1_bold.nii.gz.npy\"  \n",
    "    \n",
    "    try:  \n",
    "        interpreter = run_interpretation(model_path, data_path)  \n",
    "        print(\"Successfully completed all interpretability analyses\")  \n",
    "    except Exception as e:  \n",
    "        print(f\"An error occurred during interpretation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compute_shap_values(self, input_data):  \n",
    "        try:  \n",
    "            # 将数据移到CPU并转换为numpy数组  \n",
    "            input_data_cpu = input_data.cpu().numpy()  \n",
    "            \n",
    "            # 获取原始形状以便后续恢复  \n",
    "            original_shape = input_data_cpu.shape  \n",
    "            \n",
    "            # 展平数据为2维数组: (样本数, 特征数)  \n",
    "            # 将所有空间维度展平为一个向量  \n",
    "            flattened_data = input_data_cpu.reshape(original_shape[0], -1)  \n",
    "            \n",
    "            print(f\"Flattened data shape: {flattened_data.shape}\")  # 调试信息  \n",
    "            \n",
    "            # 创建模型包装器  \n",
    "            def model_wrapper(x):  \n",
    "                # 重塑输入回原始维度  \n",
    "                x_reshaped = torch.tensor(  \n",
    "                    x.reshape(-1, *original_shape[1:]),  # 使用原始形状  \n",
    "                    dtype=torch.float32,  \n",
    "                    device='cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "                )  \n",
    "                \n",
    "                with torch.no_grad():  \n",
    "                    output = self.model(x_reshaped)  \n",
    "                    return output.cpu().numpy()  \n",
    "            \n",
    "            # 创建背景数据（可以使用输入数据的均值或零值）  \n",
    "            background = np.zeros((1, flattened_data.shape[1]))  \n",
    "            \n",
    "            # 创建解释器  \n",
    "            explainer = shap.KernelExplainer(  \n",
    "                model_wrapper,  \n",
    "                background,  \n",
    "                link=\"identity\"  \n",
    "            )  \n",
    "            \n",
    "            # 计算SHAP值  \n",
    "            shap_values = explainer.shap_values(  \n",
    "                flattened_data,  \n",
    "                nsamples=100,  # 减少样本数以避免内存问题  \n",
    "                batch_size=1    # 使用较小的批量大小  \n",
    "            )  \n",
    "            \n",
    "            # 如果shap_values是列表（多类别情况），处理每个类别的SHAP值  \n",
    "            if isinstance(shap_values, list):  \n",
    "                shap_values = [  \n",
    "                    sv.reshape(original_shape) for sv in shap_values  \n",
    "                ]  \n",
    "            else:  \n",
    "                # 单类别情况  \n",
    "                shap_values = shap_values.reshape(original_shape)  \n",
    "            \n",
    "            print(\"SHAP values computed successfully\")  \n",
    "            return shap_values  \n",
    "            \n",
    "        except Exception as e:  \n",
    "            print(f\"Detailed error in SHAP computation: {str(e)}\")  \n",
    "            print(f\"Error type: {type(e)}\")  \n",
    "            import traceback  \n",
    "            print(traceback.format_exc())  \n",
    "            return None  \n",
    "        \n",
    "    def compute_shap_values_batch(self, background_data=None, n_samples=5, batch_size=1):  \n",
    "        try:  \n",
    "            print(\"Computing SHAP values using KernelExplainer...\")  \n",
    "            self.model.eval()  \n",
    "            \n",
    "            # 包装函数修改  \n",
    "            def model_wrapper(x):  \n",
    "                if isinstance(x, np.ndarray):  \n",
    "                    # 如果x是展平的，需要重塑回原始维度  \n",
    "                    if len(x.shape) == 2:  \n",
    "                        x = x.reshape(-1, *self.data.shape[1:])  \n",
    "                    x = torch.FloatTensor(x)  \n",
    "                    if self.data.is_cuda:  \n",
    "                        x = x.cuda()  \n",
    "                \n",
    "                with torch.no_grad():  \n",
    "                    output = self.model(x)  \n",
    "                    probs = torch.softmax(output, dim=1)  \n",
    "                    return probs.cpu().numpy()  \n",
    "            \n",
    "            # 准备背景数据  \n",
    "            if background_data is None:  \n",
    "                background_data = torch.randn(  \n",
    "                    (min(n_samples, 3),) + tuple(self.data.shape[1:]),  # 减少背景样本数量  \n",
    "                    device='cpu'  # 先放在 CPU 上  \n",
    "                )   \n",
    "            \n",
    "            # 转换并展平数据  \n",
    "            background_numpy = background_data.cpu().numpy()  \n",
    "            data_numpy = self.data.cpu().numpy()  \n",
    "            \n",
    "            # 展平数据为2维  \n",
    "            background_flat = background_numpy.reshape(background_numpy.shape[0], -1)  \n",
    "            data_flat = data_numpy.reshape(data_numpy.shape[0], -1)  \n",
    "            \n",
    "            print(f\"Flattened data shape: {data_flat.shape}\")  \n",
    "            \n",
    "            try:  \n",
    "                explainer = shap.KernelExplainer(  \n",
    "                    model_wrapper,  \n",
    "                    background_flat,  # 使用展平的背景数据  \n",
    "                    link=\"identity\"  \n",
    "                )  \n",
    "                print(\"KernelExplainer created successfully\")  \n",
    "            except Exception as e:  \n",
    "                print(f\"Error creating KernelExplainer: {str(e)}\")  \n",
    "                return None  \n",
    "            \n",
    "            # 分批计算SHAP值  \n",
    "            all_shap_values = []  \n",
    "            total_batches = (len(data_flat) + batch_size - 1) // batch_size  \n",
    "            \n",
    "            for i in tqdm(range(0, len(data_flat), batch_size), total=total_batches, desc=\"Computing SHAP values\"):  \n",
    "                # print(f\"Processing batch {i//batch_size + 1}/{total_batches}\")  \n",
    "                \n",
    "                try:  \n",
    "                    batch = data_flat[i:i + batch_size]  # 使用展平的数据  \n",
    "                    batch_shap_values = explainer.shap_values(  \n",
    "                        batch,  \n",
    "                        nsamples=50  \n",
    "                    )  \n",
    "                    \n",
    "                    # 重塑SHAP值回原始维度  \n",
    "                    if isinstance(batch_shap_values, list):  \n",
    "                        # 多类别情况  \n",
    "                        batch_shap_values = [  \n",
    "                            sv.reshape(-1, *self.data.shape[1:])  \n",
    "                            for sv in batch_shap_values  \n",
    "                        ]  \n",
    "                    else:  \n",
    "                        # 单类别情况  \n",
    "                        batch_shap_values = batch_shap_values.reshape(-1, *self.data.shape[1:])  \n",
    "                    \n",
    "                    all_shap_values.append(batch_shap_values)  \n",
    "                    print(f\"Batch {i//batch_size + 1} completed\")  \n",
    "                    \n",
    "                except Exception as e:  \n",
    "                    print(f\"Error processing batch {i//batch_size + 1}: {str(e)}\")  \n",
    "                    continue  \n",
    "                \n",
    "                if self.data.is_cuda:  \n",
    "                    torch.cuda.empty_cache()  \n",
    "            \n",
    "            if not all_shap_values:  \n",
    "                print(\"No SHAP values were successfully computed\")  \n",
    "                return None  \n",
    "            \n",
    "            # 合并结果  \n",
    "            try:  \n",
    "                if isinstance(all_shap_values[0], list):  \n",
    "                    final_shap_values = [  \n",
    "                        np.concatenate([batch[i] for batch in all_shap_values])  \n",
    "                        for i in range(len(all_shap_values[0]))  \n",
    "                    ]  \n",
    "                else:  \n",
    "                    final_shap_values = np.concatenate(all_shap_values)  \n",
    "                \n",
    "                print(\"SHAP values computation completed\")  \n",
    "                return final_shap_values  \n",
    "                \n",
    "            except Exception as e:  \n",
    "                print(f\"Error merging results: {str(e)}\")  \n",
    "                return None  \n",
    "            \n",
    "        except Exception as e:  \n",
    "            print(f\"Error in compute_shap_values_batch: {str(e)}\")  \n",
    "            import traceback  \n",
    "            traceback.print_exc()  \n",
    "            return None\n",
    "\n",
    "    def _reshape_data_for_shap(self, data):  \n",
    "        \"\"\"  \n",
    "        重塑数据以适应SHAP分析  \n",
    "        \"\"\"  \n",
    "        # 假设输入形状为 (batch, time, channel, depth, height, width)  \n",
    "        # 或 (batch, channel, depth, height, width)  \n",
    "        shape = data.shape  \n",
    "        if len(shape) == 6:  # 6D数据  \n",
    "            # 将时间维度展平到批次维度  \n",
    "            return data.reshape(-1, *shape[2:])  \n",
    "        return data  \n",
    "    \n",
    "    def _plot_shap_summary(self,   \n",
    "                          shap_values: np.ndarray,  \n",
    "                          save_path: str,  \n",
    "                          feature_names: Optional[List[str]] = None):  \n",
    "        \"\"\"  \n",
    "        绘制SHAP值的总结图  \n",
    "        \"\"\"  \n",
    "        plt.figure(figsize=(12, 8))  \n",
    "        \n",
    "        # 如果是多分类，取第一个类别的SHAP值  \n",
    "        if isinstance(shap_values, list):  \n",
    "            shap_values = shap_values[0]  \n",
    "        \n",
    "        # 计算每个特征的平均绝对SHAP值  \n",
    "        feature_importance = np.mean(np.abs(shap_values), axis=0)  \n",
    "        \n",
    "        # 创建特征名称  \n",
    "        if feature_names is None:  \n",
    "            feature_names = [f'Feature {i}' for i in range(len(feature_importance))]  \n",
    "        \n",
    "        # 绘制条形图  \n",
    "        plt.barh(range(len(feature_importance)), feature_importance)  \n",
    "        plt.yticks(range(len(feature_importance)), feature_names)  \n",
    "        plt.xlabel('mean(|SHAP value|)')  \n",
    "        plt.title('Feature Importance Based on SHAP Values')  \n",
    "        \n",
    "        plt.tight_layout()  \n",
    "        plt.savefig(save_path)  \n",
    "        plt.close()  \n",
    "    \n",
    "    def _plot_shap_3d_heatmap(self,   \n",
    "                             shap_values: np.ndarray,  \n",
    "                             save_path: str,  \n",
    "                             original_data: Optional[np.ndarray] = None):  \n",
    "        \"\"\"  \n",
    "        绘制3D SHAP值热力图  \n",
    "        \"\"\"  \n",
    "        # 确保SHAP值是3D的  \n",
    "        if len(shap_values.shape) > 3:  \n",
    "            shap_values = np.mean(shap_values, axis=0)  \n",
    "        \n",
    "        # 调用原有的_plot_3d_heatmap方法  \n",
    "        self._plot_3d_heatmap(  \n",
    "            data=shap_values,  \n",
    "            save_path=save_path,  \n",
    "            title='SHAP Values 3D Visualization',  \n",
    "            original_data=original_data  \n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
